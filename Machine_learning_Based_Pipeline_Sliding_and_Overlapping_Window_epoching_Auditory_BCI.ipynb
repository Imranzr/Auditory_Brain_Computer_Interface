{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMIerwWYJ11pUER8w9Rwxwg"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["pip install mne"],"metadata":{"id":"jv3nnSXIMu0I"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pip install pyriemann"],"metadata":{"id":"PGxeXDFPMxnn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import argparse\n","import sys\n","import mne\n","import math\n","import time\n","import json\n","import numpy as np\n","from scipy.signal import butter, filtfilt\n","from pyriemann.estimation import XdawnCovariances\n","from pyriemann.tangentspace import TangentSpace\n","from sklearn.pipeline import make_pipeline\n","from sklearn.model_selection import StratifiedKFold\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import classification_report, matthews_corrcoef\n","\n","start = time.time()\n","\n","\n","def is_notebook():\n","    try:\n","        shell = get_ipython().__class__.__name__\n","        if shell == 'ZMQInteractiveShell':\n","            return True\n","        elif shell == 'TerminalInteractiveShell':\n","            return False\n","        else:\n","            return False\n","    except NameError:\n","        return False\n","\n","if is_notebook():\n","    args = argparse.Namespace(s=None, c=None)\n","else:\n","    parser = argparse.ArgumentParser()\n","    parser.add_argument('-s', default=None)\n","    parser.add_argument('-c', default=None, type=int)\n","\n","    args = parser.parse_args(args=[])\n","\n","print(args.s)\n","print(args.c)\n","print(__doc__)\n","\n","subject = 'sub-B'\n","if args.s is not None:\n","    subject = args.s\n","test_class =1\n","if args.c is not None:\n","    test_class = args.c\n","\n","import numpy as np\n","fnum = np.array([[1,4],\n","                 [2,5],\n","                 [3,6]])\n","\n","trig_id = [2,8,32]\n","tasks = ['low', 'low', 'mid', 'mid', 'high', 'high']\n","reject={'eeg':100e-6,'eog':500e-6}\n","\n","import os\n","import json\n","repository_base = os.path.dirname(os.path.dirname(os.path.abspath('file_path')))\n","base = os.path.join(repository_base, \"eeg\")\n","save_base = os.path.join(repository_base, \"results\")\n","if not os.path.exists(save_base):\n","    os.makedirs(save_base)\n","\n","Fs = 1000\n","fc = [1, 40]\n","resample = None\n","\n","from scipy.signal import butter, filtfilt\n","def apply_filter(data, b, a):\n","    r = filtfilt(b=b, a=a, x=data)\n","    return r\n","b,a = butter(N = 2, Wn = np.array(fc)/(Fs/2), btype = 'bandpass', output = 'ba')\n","\n","t_file = []\n","nt_file = []\n","target_file = []\n","non_target_file = []\n","\n","for i in range(len(fnum.ravel())):\n","    fname = os.path.join(base, subject, \"eeg\", \"%s_task-%s_run-%d_eeg.vhdr\" % (subject, tasks[i], fnum.ravel()[i]))\n","    print(fname)\n","    if np.any(fnum[test_class-1] == fnum.ravel()[i]):\n","        if isinstance(target_file, list):\n","            target_file = mne.io.read_raw_brainvision(fname, preload=True, eog=('hEOG', 'vEOG'))\n","            target_file = target_file.apply_function(apply_filter, channel_wise=True, b=b, a=a)\n","            t_file.append(fnum.ravel()[i])\n","        else:\n","            tmp = mne.io.read_raw_brainvision(fname, preload=True, eog=('hEOG', 'vEOG'))\n","            tmp = tmp.apply_function(apply_filter, channel_wise=True, b=b, a=a)\n","            target_file = mne.concatenate_raws([target_file, tmp])\n","            t_file.append(fnum.ravel()[i])\n","    else:\n","        if isinstance(non_target_file, list):\n","            non_target_file = mne.io.read_raw_brainvision(fname, preload=True, eog=('hEOG', 'vEOG'))\n","            non_target_file = non_target_file.apply_function(apply_filter, channel_wise=True, b=b, a=a)\n","            nt_file.append(fnum.ravel()[i])\n","        else:\n","            tmp = mne.io.read_raw_brainvision(fname, preload=True, eog=('hEOG', 'vEOG'))\n","            tmp = tmp.apply_function(apply_filter, channel_wise=True, b=b, a=a)\n","            non_target_file = mne.concatenate_raws([non_target_file, tmp])\n","            nt_file.append(fnum.ravel()[i])\n","if resample is not None:\n","    target_file.resample(resample)\n","    non_target_file.resample(resample)\n","\n","if resample != None:\n","    target_file.resample(resample)\n","    non_target_file.resample(resample)\n","\n","event_id={'target_stimulus_id':-100,'non_target_stimulus_id':-500}\n","target_eve = mne.events_from_annotations(target_file)\n","target_eve = mne.merge_events(target_eve[0],[trig_id[test_class-1]],event_id['target_stimulus_id'],replace_events=True)\n","\n","non_target_eve = mne.events_from_annotations(non_target_file)\n","non_target_eve = mne.merge_events(non_target_eve[0],[trig_id[test_class-1]],event_id['non_target_stimulus_id'],replace_events=True)\n","\n","tmin,tmax= -0.02, 1.4\n","baseline=(0.0,0.01)\n","target_epochs = mne.Epochs(target_file,events=target_eve, event_id=event_id['target_stimulus_id'], tmin=tmin,tmax=tmax, baseline=baseline, reject=reject,preload = True)"],"metadata":{"id":"4d9TobesM0OB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Sliding and Overlapping Window Epoching"],"metadata":{"id":"IIbFrsLmY0um"}},{"cell_type":"code","source":["\n","import numpy as np\n","def find_max_peak_window(data, Fs, window_size_ms=500, overlap_size_ms=100, epoch_index=0):\n","    \"\"\"\n","    Function to find the window with the maximum peak value within a specific epoch and return its start and end indices.\n","    \"\"\"\n","    window_size_samples = int(window_size_ms * Fs / 1000)\n","    overlap_size_samples = int(overlap_size_ms * Fs / 1000)\n","    epoch_data = data[epoch_index]\n","    n_channels, n_times = epoch_data.shape\n","    max_peak_value = -np.inf\n","    max_peak_start_index = None\n","    max_peak_end_index = None\n","    start = 0\n","    while start + window_size_samples <= n_times:\n","        window_data = epoch_data[:, start:start + window_size_samples]\n","        current_peak_value = np.max(window_data)\n","        if current_peak_value > max_peak_value:\n","            max_peak_value = current_peak_value\n","            max_peak_start_index = start\n","            max_peak_end_index = start + window_size_samples - 1\n","        start += (window_size_samples - overlap_size_samples)\n","    print(f\"Epoch {epoch_index + 1} - Max Peak Value: {max_peak_value}\")\n","    print(f\"Epoch {epoch_index + 1} - Window Start Index: {max_peak_start_index}, End Index: {max_peak_end_index}\")\n","    return max_peak_start_index, max_peak_end_index\n","\n","def analyze_all_epochs_max_peaks(data, Fs, window_size_ms=500, overlap_size_ms=100):\n","    \"\"\"\n","    Function to find and print the maximum peak values and corresponding window indices across all epochs.\n","    \"\"\"\n","    n_epochs = data.shape[0]\n","    peak_window_indices = []\n","    for epoch_index in range(n_epochs):\n","        print(f\"\\nAnalyzing Epoch {epoch_index + 1}\")\n","        start_index, end_index = find_max_peak_window(data, Fs, window_size_ms, overlap_size_ms, epoch_index)\n","        peak_window_indices.append((start_index, end_index))\n","    return peak_window_indices\n","\n","def re_epoch_data(data, peak_window_indices):\n","    \"\"\"\n","    Function to re-epoch the data based on the peak window indices.\n","    \"\"\"\n","    re_epoched_data = []\n","    for epoch_index, (start, end) in enumerate(peak_window_indices):\n","        re_epoched_epoch_data = data[epoch_index][:, start:end + 1]\n","        re_epoched_data.append(re_epoched_epoch_data)\n","    re_epoched_data = np.array(re_epoched_data)\n","    print(\"\\nRe-epoching complete.\")\n","    print(\"Re-epoched data shape:\", re_epoched_data.shape)\n","    return re_epoched_data\n","\n","Fs = 1000\n","data = target_epochs.get_data()\n","peak_window_indices = analyze_all_epochs_max_peaks(data, Fs, window_size_ms=500, overlap_size_ms=100)\n","re_epoched_data = re_epoch_data(data, peak_window_indices)\n","\n","from mne import EpochsArray\n","from mne import create_info\n","n_channels = re_epoched_data.shape[1]\n","info = create_info(ch_names=[f'chan{i}' for i in range(n_channels)], sfreq=Fs, ch_types='eeg')\n","re_epoched_epochs = EpochsArray(re_epoched_data, info)\n","print(\"Re-epoched data converted to MNE Epochs:\", re_epoched_epochs)\n","\n","\n","\n","from mne import EpochsArray, create_info\n","n_channels = re_epoched_data.shape[1]\n","info = create_info(ch_names=[f'chan{i}' for i in range(n_channels)], sfreq=Fs, ch_types='eeg')\n","re_epoched_epochs = EpochsArray(re_epoched_data, info)\n","baseline=(0.0,0.01)\n","re_epoched_epochs.apply_baseline(baseline)\n","re_epoched_epochs.filter(l_freq=0.50, h_freq=100.00)\n","print(\"Re-epoched data converted to MNE Epochs and processed with baseline correction and filtering.\")\n","\n","tmin,tmax= -0.0, 0.499\n","baseline=(0.0,0.01)\n","non_target_epochs = mne.Epochs(non_target_file, events=non_target_eve, event_id=event_id['non_target_stimulus_id'], tmin=tmin,tmax=tmax, baseline=baseline, reject=reject,preload = True)\n","\n","\n","import mne\n","tmin, tmax = -0.0, 0.499\n","baseline=(0.0,0.01)\n","non_target_epochs = mne.Epochs(\n","    non_target_file,\n","    events=non_target_eve,\n","    event_id=event_id['non_target_stimulus_id'],\n","    tmin=tmin,\n","    tmax=tmax,\n","    baseline=baseline,\n","    reject=reject,\n","    preload=True\n",")\n","\n","new_channel_names = [f'chan{i}' for i in range(len(non_target_epochs.ch_names))]\n","rename_dict = dict(zip(non_target_epochs.ch_names, new_channel_names))\n","non_target_epochs.rename_channels(rename_dict)\n","print(\"Updated channel names:\", non_target_epochs.ch_names)\n","\n"],"metadata":{"id":"Ca9n5f42REh5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["epochs = mne.concatenate_epochs([re_epoched_epochs, non_target_epochs])\n","epochs = epochs.copy().pick_types(eeg=True, eog=False)"],"metadata":{"id":"VdwmEZ0wYzKU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Logistic Regression"],"metadata":{"id":"nF7AR3l1WwDs"}},{"cell_type":"code","source":["\n","clf = make_pipeline(XdawnCovariances(8),\n","                    TangentSpace(metric='logeuclid'),\n","                    LogisticRegression( penalty='l1', solver='liblinear', multi_class='ovr'))\n","\n","epochs_data = epochs.get_data()\n","labels = epochs.events[:, -1]\n","preds = np.zeros(len(labels))\n","\n","cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n","\n","preds = np.empty(len(labels))\n","for train_idx, test_idx in cv.split(epochs_data, labels):\n","    clf.fit(epochs_data[train_idx], labels[train_idx])\n","    preds[test_idx] = clf.predict(epochs_data[test_idx])\n","\n","report = classification_report(labels, preds, target_names=['non-target', 'target'], output_dict=True)\n","print(report)\n"],"metadata":{"id":"XbmzPgFcQ7yk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Support Vector Machine (SVM)"],"metadata":{"id":"j2Z_8QXoW2YP"}},{"cell_type":"code","source":["from sklearn.svm import SVC\n","from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","clf = make_pipeline(XdawnCovariances(8),\n","                    TangentSpace(metric='logeuclid'),\n","                    SVC(kernel='linear', C=1, decision_function_shape='ovr'))\n","\n","\n","epochs_data = epochs.get_data()\n","labels = epochs.events[:, -1]\n","preds = np.zeros(len(labels))\n","\n","cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n","\n","train_accuracies = []\n","test_accuracies = []\n","train_losses = []\n","test_losses = []\n","preds = np.empty(len(labels))\n","for train_idx, test_idx in cv.split(epochs_data, labels):\n","    clf.fit(epochs_data[train_idx], labels[train_idx])\n","    train_preds = clf.predict(epochs_data[train_idx])\n","    test_preds = clf.predict(epochs_data[test_idx])\n","    train_accuracy = accuracy_score(labels[train_idx], train_preds)\n","    test_accuracy = accuracy_score(labels[test_idx], test_preds)\n","    train_accuracies.append(train_accuracy)\n","    test_accuracies.append(test_accuracy)\n","    train_loss = np.mean(train_preds != labels[train_idx])\n","    test_loss = np.mean(test_preds != labels[test_idx])\n","    train_losses.append(train_loss)\n","    test_losses.append(test_loss)\n","    preds[test_idx] = test_preds\n","report = classification_report(labels, preds, target_names=['non-target', 'target'], output_dict=True)\n","print(report)\n","\n","\n","epochs_range = range(1, len(train_accuracies) + 1)\n","fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n","ax1.plot(epochs_range, train_accuracies, label='Train Accuracy', marker='o')\n","ax1.plot(epochs_range, test_accuracies, label='Test Accuracy', marker='x')\n","ax1.set_title('Train and Test Accuracy')\n","ax1.set_xlabel('Fold')\n","ax1.set_ylabel('Accuracy')\n","ax1.legend()\n","\n","\n","for i in range(len(train_accuracies)):\n","    ax1.annotate(f'{train_accuracies[i]:.2f}', (epochs_range[i], train_accuracies[i]),\n","                 textcoords=\"offset points\", xytext=(0, 10), ha='center', fontsize=8)\n","    ax1.annotate(f'{test_accuracies[i]:.2f}', (epochs_range[i], test_accuracies[i]),\n","                 textcoords=\"offset points\", xytext=(0, 10), ha='center', fontsize=8)\n","\n","ax2.plot(epochs_range, train_losses, label='Train Loss', marker='o')\n","ax2.plot(epochs_range, test_losses, label='Test Loss', marker='x')\n","ax2.set_title('Train and Test Loss')\n","ax2.set_xlabel('Fold')\n","ax2.set_ylabel('Loss')\n","ax2.legend()\n","\n","for i in range(len(train_losses)):\n","    ax2.annotate(f'{train_losses[i]:.2f}', (epochs_range[i], train_losses[i]),\n","                 textcoords=\"offset points\", xytext=(0, 10), ha='center', fontsize=8)\n","    ax2.annotate(f'{test_losses[i]:.2f}', (epochs_range[i], test_losses[i]),\n","                 textcoords=\"offset points\", xytext=(0, 10), ha='center', fontsize=8)\n","\n","plt.tight_layout()\n","plt.show()\n","cm = confusion_matrix(labels, preds)\n","plt.figure(figsize=(6, 5))\n","sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=['non-target', 'target'], yticklabels=['non-target', 'target'])\n","plt.title('Confusion Matrix')\n","plt.xlabel('Predicted')\n","plt.ylabel('True')\n","plt.show()\n"],"metadata":{"id":"_0qB57AwW0hR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Random Forest"],"metadata":{"id":"ADEK5YAuW-WG"}},{"cell_type":"code","source":["from sklearn.ensemble import RandomForestClassifier\n","\n","clf = make_pipeline(XdawnCovariances(8),\n","                    TangentSpace(metric='logeuclid'),\n","                    RandomForestClassifier(n_estimators=100, random_state=42))\n","\n","epochs_data = epochs.get_data()\n","labels = epochs.events[:, -1]\n","preds = np.zeros(len(labels))\n","\n","cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n","\n","train_accuracies = []\n","test_accuracies = []\n","train_losses = []\n","test_losses = []\n","\n","preds = np.empty(len(labels))\n","for train_idx, test_idx in cv.split(epochs_data, labels):\n","\n","    clf.fit(epochs_data[train_idx], labels[train_idx])\n","    train_preds = clf.predict(epochs_data[train_idx])\n","    test_preds = clf.predict(epochs_data[test_idx])\n","    train_accuracy = accuracy_score(labels[train_idx], train_preds)\n","    test_accuracy = accuracy_score(labels[test_idx], test_preds)\n","    train_accuracies.append(train_accuracy)\n","    test_accuracies.append(test_accuracy)\n","    train_loss = np.mean(train_preds != labels[train_idx])\n","    test_loss = np.mean(test_preds != labels[test_idx])\n","\n","    train_losses.append(train_loss)\n","    test_losses.append(test_loss)\n","    preds[test_idx] = test_preds\n","\n","report = classification_report(labels, preds, target_names=['non-target', 'target'], output_dict=True)\n","print(report)\n","\n","\n","epochs_range = range(1, len(train_accuracies) + 1)\n","fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n","\n","ax1.plot(epochs_range, train_accuracies, label='Train Accuracy', marker='o')\n","ax1.plot(epochs_range, test_accuracies, label='Test Accuracy', marker='x')\n","ax1.set_title('Train and Test Accuracy')\n","ax1.set_xlabel('Fold')\n","ax1.set_ylabel('Accuracy')\n","ax1.legend()\n","\n","\n","for i in range(len(train_accuracies)):\n","    ax1.annotate(f'{train_accuracies[i]:.2f}', (epochs_range[i], train_accuracies[i]),\n","                 textcoords=\"offset points\", xytext=(0, 10), ha='center', fontsize=8)\n","    ax1.annotate(f'{test_accuracies[i]:.2f}', (epochs_range[i], test_accuracies[i]),\n","                 textcoords=\"offset points\", xytext=(0, 10), ha='center', fontsize=8)\n","\n","ax2.plot(epochs_range, train_losses, label='Train Loss', marker='o')\n","ax2.plot(epochs_range, test_losses, label='Test Loss', marker='x')\n","ax2.set_title('Train and Test Loss')\n","ax2.set_xlabel('Fold')\n","ax2.set_ylabel('Loss')\n","ax2.legend()\n","\n","for i in range(len(train_losses)):\n","    ax2.annotate(f'{train_losses[i]:.2f}', (epochs_range[i], train_losses[i]),\n","                 textcoords=\"offset points\", xytext=(0, 10), ha='center', fontsize=8)\n","    ax2.annotate(f'{test_losses[i]:.2f}', (epochs_range[i], test_losses[i]),\n","                 textcoords=\"offset points\", xytext=(0, 10), ha='center', fontsize=8)\n","\n","plt.tight_layout()\n","plt.show()\n","\n","cm = confusion_matrix(labels, preds)\n","plt.figure(figsize=(6, 5))\n","sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=['non-target', 'target'], yticklabels=['non-target', 'target'])\n","plt.title('Confusion Matrix')\n","plt.xlabel('Predicted')\n","plt.ylabel('True')\n","plt.show()\n"],"metadata":{"id":"CXNqmDXQW-AI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Polynomial Logistic Regression"],"metadata":{"id":"tMVovSgXXDcm"}},{"cell_type":"code","source":["from sklearn.preprocessing import PolynomialFeatures\n","from sklearn.linear_model import LogisticRegression\n","\n","clf = make_pipeline(XdawnCovariances(8),\n","                    TangentSpace(metric='logeuclid'),\n","                    PolynomialFeatures(degree=2, include_bias=False),\n","                    LogisticRegression(penalty='l2', solver='liblinear', multi_class='ovr'))\n","\n","epochs_data = epochs.get_data()\n","labels = epochs.events[:, -1]\n","preds = np.zeros(len(labels))\n","\n","cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n","train_accuracies = []\n","test_accuracies = []\n","train_losses = []\n","test_losses = []\n","\n","preds = np.empty(len(labels))\n","for train_idx, test_idx in cv.split(epochs_data, labels):\n","    clf.fit(epochs_data[train_idx], labels[train_idx])\n","    train_preds = clf.predict(epochs_data[train_idx])\n","    test_preds = clf.predict(epochs_data[test_idx])\n","    train_accuracy = accuracy_score(labels[train_idx], train_preds)\n","    test_accuracy = accuracy_score(labels[test_idx], test_preds)\n","    train_accuracies.append(train_accuracy)\n","    test_accuracies.append(test_accuracy)\n","    train_loss = np.mean(train_preds != labels[train_idx])\n","    test_loss = np.mean(test_preds != labels[test_idx])\n","    train_losses.append(train_loss)\n","    test_losses.append(test_loss)\n","    preds[test_idx] = test_preds\n","\n","\n","report = classification_report(labels, preds, target_names=['non-target', 'target'], output_dict=True)\n","print(report)\n","\n","epochs_range = range(1, len(train_accuracies) + 1)\n","fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n","\n","ax1.plot(epochs_range, train_accuracies, label='Train Accuracy', marker='o')\n","ax1.plot(epochs_range, test_accuracies, label='Test Accuracy', marker='x')\n","ax1.set_title('Train and Test Accuracy')\n","ax1.set_xlabel('Fold')\n","ax1.set_ylabel('Accuracy')\n","ax1.legend()\n","\n","for i in range(len(train_accuracies)):\n","    ax1.annotate(f'{train_accuracies[i]:.2f}', (epochs_range[i], train_accuracies[i]),\n","                 textcoords=\"offset points\", xytext=(0, 10), ha='center', fontsize=8)\n","    ax1.annotate(f'{test_accuracies[i]:.2f}', (epochs_range[i], test_accuracies[i]),\n","                 textcoords=\"offset points\", xytext=(0, 10), ha='center', fontsize=8)\n","ax2.plot(epochs_range, train_losses, label='Train Loss', marker='o')\n","ax2.plot(epochs_range, test_losses, label='Test Loss', marker='x')\n","ax2.set_title('Train and Test Loss')\n","ax2.set_xlabel('Fold')\n","ax2.set_ylabel('Loss')\n","ax2.legend()\n","\n","\n","for i in range(len(train_losses)):\n","    ax2.annotate(f'{train_losses[i]:.2f}', (epochs_range[i], train_losses[i]),\n","                 textcoords=\"offset points\", xytext=(0, 10), ha='center', fontsize=8)\n","    ax2.annotate(f'{test_losses[i]:.2f}', (epochs_range[i], test_losses[i]),\n","                 textcoords=\"offset points\", xytext=(0, 10), ha='center', fontsize=8)\n","plt.tight_layout()\n","plt.show()\n","\n","\n","cm = confusion_matrix(labels, preds)\n","plt.figure(figsize=(6, 5))\n","sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=['non-target', 'target'], yticklabels=['non-target', 'target'])\n","plt.title('Confusion Matrix')\n","plt.xlabel('Predicted')\n","plt.ylabel('True')\n","plt.show()\n"],"metadata":{"id":"3pwbacSQXEuf"},"execution_count":null,"outputs":[]}]}
