{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPgUHcbHV/0gksfEcZVF8Vw"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["pip install mne"],"metadata":{"id":"tAkbMa3bXZlf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pip install pyriemann"],"metadata":{"id":"rCe_2nEQXYqA"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cruwlyX0M8mv"},"outputs":[],"source":["import os\n","import argparse\n","import sys\n","import mne\n","import math\n","import time\n","import json\n","import numpy as np\n","from scipy.signal import butter, filtfilt\n","from pyriemann.estimation import XdawnCovariances\n","from pyriemann.tangentspace import TangentSpace\n","from sklearn.pipeline import make_pipeline\n","from sklearn.model_selection import StratifiedKFold\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import classification_report, matthews_corrcoef\n","\n","start = time.time()\n","\n","def is_notebook():\n","    try:\n","        shell = get_ipython().__class__.__name__\n","        if shell == 'ZMQInteractiveShell':\n","            return True\n","        elif shell == 'TerminalInteractiveShell':\n","            return False\n","        else:\n","            return False\n","    except NameError:\n","        return False\n","\n","if is_notebook():\n","\n","    args = argparse.Namespace(s=None, c=None)\n","else:\n","    parser = argparse.ArgumentParser()\n","    parser.add_argument('-s', default=None)\n","    parser.add_argument('-c', default=None, type=int)\n","    args = parser.parse_args(args=[])\n","\n","print(args.s)\n","print(args.c)\n","print(__doc__)\n","\n","subject = 'sub-B'\n","if args.s is not None:\n","    subject = args.s\n","test_class =1\n","if args.c is not None:\n","    test_class = args.c\n","\n","import numpy as np\n","fnum = np.array([[1,4],\n","                 [2,5],\n","                 [3,6]])\n","\n","trig_id = [2,8,32]\n","tasks = ['low', 'low', 'mid', 'mid', 'high', 'high']\n","reject={'eeg':100e-6,'eog':500e-6}\n","\n","import os\n","import json\n","\n","repository_base = os.path.dirname(os.path.dirname(os.path.abspath('file_path')))\n","base = os.path.join(repository_base, \"eeg\")\n","save_base = os.path.join(repository_base, \"results\")\n","if not os.path.exists(save_base):\n","    os.makedirs(save_base)\n","\n","Fs = 1000\n","fc = [1, 40]\n","resample = None\n","\n","from scipy.signal import butter, filtfilt\n","def apply_filter(data, b, a):\n","    r = filtfilt(b=b, a=a, x=data)\n","    return r\n","b,a = butter(N = 2, Wn = np.array(fc)/(Fs/2), btype = 'bandpass', output = 'ba')\n","\n","t_file = []\n","nt_file = []\n","\n","target_file = []\n","non_target_file = []\n","\n","for i in range(len(fnum.ravel())):\n","    fname = os.path.join(base, subject, \"eeg\", \"%s_task-%s_run-%d_eeg.vhdr\" % (subject, tasks[i], fnum.ravel()[i]))\n","    print(fname)\n","    if np.any(fnum[test_class-1] == fnum.ravel()[i]):\n","        if isinstance(target_file, list):\n","            target_file = mne.io.read_raw_brainvision(fname, preload=True, eog=('hEOG', 'vEOG'))\n","            target_file = target_file.apply_function(apply_filter, channel_wise=True, b=b, a=a)\n","            t_file.append(fnum.ravel()[i])\n","        else:\n","            tmp = mne.io.read_raw_brainvision(fname, preload=True, eog=('hEOG', 'vEOG'))\n","            tmp = tmp.apply_function(apply_filter, channel_wise=True, b=b, a=a)\n","            target_file = mne.concatenate_raws([target_file, tmp])\n","            t_file.append(fnum.ravel()[i])\n","    else:\n","        if isinstance(non_target_file, list):\n","            non_target_file = mne.io.read_raw_brainvision(fname, preload=True, eog=('hEOG', 'vEOG'))\n","            non_target_file = non_target_file.apply_function(apply_filter, channel_wise=True, b=b, a=a)\n","            nt_file.append(fnum.ravel()[i])\n","        else:\n","            tmp = mne.io.read_raw_brainvision(fname, preload=True, eog=('hEOG', 'vEOG'))\n","            tmp = tmp.apply_function(apply_filter, channel_wise=True, b=b, a=a)\n","            non_target_file = mne.concatenate_raws([non_target_file, tmp])\n","            nt_file.append(fnum.ravel()[i])\n","if resample is not None:\n","    target_file.resample(resample)\n","    non_target_file.resample(resample)\n","\n","if resample != None:\n","    target_file.resample(resample)\n","    non_target_file.resample(resample)\n","\n","target_eve = mne.events_from_annotations(target)\n","non_target_eve = mne.events_from_annotations(non_target)\n","\n","target_eve = mne.merge_events(target_eve[0], [trig_id[test_class-1]], event_id['target'], replace_events=True)\n","non_target_eve = mne.merge_events(non_target_eve[0], [trig_id[test_class-1]], event_id['non_target'], replace_events=True)\n","\n","epochs_target = mne.Epochs(target, events=target_eve, event_id=event_id['target'],\n","                            tmin=tmin, tmax=tmax, baseline=baseline,\n","                           reject=reject, preload=True)\n","\n","for idx, event in enumerate(epochs_target.events):\n","    if event[2] == -100:\n","        annot_time = event[0] / epochs_target.info['sfreq']\n","        epochs_target.annotations.append(\n","            onset=annot_time, duration=0.001, description=\"target_event -100\"\n","        )\n"]},{"cell_type":"markdown","source":["# Dynamical_Window_Epoching"],"metadata":{"id":"Zlz0_sL-YRk8"}},{"cell_type":"code","source":["latencies_p300 = []\n","target_epochs_dynamic = []\n","non_target_epochs_dynamic = []\n","for epoch_idx, epoch in enumerate(epochs_target):\n","    picks = mne.pick_types(epochs_target.info, eeg=True, exclude='bads')\n","    data = epoch[picks].mean(axis=0)\n","    times = epochs_target.times\n","    p300_time_window = (0.3, 1.4)\n","    idx_start = np.searchsorted(times, p300_time_window[0])\n","    idx_end = np.searchsorted(times, p300_time_window[1])\n","    if idx_end > idx_start:\n","        data_window = data[idx_start:idx_end]\n","        peak_idx = np.argmax(data_window)\n","        peak_time = times[idx_start + peak_idx]\n","        latency_p300 = peak_time\n","    else:\n","        latency_p300 = None\n","    latencies_p300.append(latency_p300)\n","    if latency_p300 is not None:\n","        print(f\"Epoch {epoch_idx + 1}: P300 latency = {latency_p300:.3f} seconds\")\n","    else:\n","        print(f\"Epoch {epoch_idx + 1}: P300 latency not found\")\n","    if latency_p300 is not None:\n","        tmin_dynamic = max(-0.38, latency_p300 - 0.33)\n","        tmax_dynamic = latency_p300 + 0.33\n","        baseline_start = max(tmin_dynamic, baseline[0])\n","        baseline_end = min(tmax_dynamic, baseline[1])\n","        if baseline_start >= baseline_end:\n","            print(f\"Adjusting baseline to fit within the epoch window: [{tmin_dynamic}, {tmax_dynamic}] s\")\n","            baseline_start = tmin_dynamic\n","            baseline_end = tmin_dynamic + 0.045\n","        target_epoch_dynamic = mne.Epochs(target, events=target_eve, event_id=event_id['target'],\n","                                          tmin=tmin_dynamic, tmax=tmax_dynamic,\n","                                          baseline=(baseline_start, baseline_end),\n","                                          reject=reject, preload=True)\n","        non_target_epoch_dynamic = mne.Epochs(non_target, events=non_target_eve, event_id=event_id['non_target'],\n","                                              tmin=tmin_dynamic, tmax=tmax_dynamic,\n","                                              baseline=(baseline_start, baseline_end),\n","                                              reject=reject, preload=True)\n","        target_epochs_dynamic.append(target_epoch_dynamic)\n","        non_target_epochs_dynamic.append(non_target_epoch_dynamic)\n","    else:\n","        print(\"P300 latency not found, using default epoch window of -0.1 to 0.5 sec\")\n","        target_epoch_default = mne.Epochs(target, events=target_eve, event_id=event_id['target'],\n","                                          tmin=tmin, tmax=tmax, baseline=baseline,\n","                                          reject=reject, preload=True)\n","        non_target_epoch_default = mne.Epochs(non_target, events=non_target_eve, event_id=event_id['non_target'],\n","                                              tmin=tmin, tmax=tmax, baseline=baseline,\n","                                              reject=reject, preload=True)\n","        target_epochs_dynamic.append(target_epoch_default)\n","        non_target_epochs_dynamic.append(non_target_epoch_default)\n","\n","print(\"P300 latencies (in seconds) for all epochs:\", latencies_p300)\n"],"metadata":{"id":"q61swNKLNx8N"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["epochs = mne.concatenate_epochs([target_epoch_dynamic, non_target_epoch_dynamic])\n","epochs = all_epochs.copy().pick_types(eeg=True, eog=False)"],"metadata":{"id":"UtymZYE_YuDg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Logistic Regression"],"metadata":{"id":"sJGvkvD0WkF8"}},{"cell_type":"code","source":["\n","\n","clf = make_pipeline(XdawnCovariances(8),\n","                    TangentSpace(metric='logeuclid'),\n","                    LogisticRegression( penalty='l1', solver='liblinear', multi_class='ovr'))\n","epochs_data = epochs.get_data()\n","labels = epochs.events[:, -1]\n","preds = np.zeros(len(labels))\n","\n","cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n","\n","preds = np.empty(len(labels))\n","for train_idx, test_idx in cv.split(epochs_data, labels):\n","    clf.fit(epochs_data[train_idx], labels[train_idx])\n","    preds[test_idx] = clf.predict(epochs_data[test_idx])\n","\n","report = classification_report(labels, preds, target_names=['non-target', 'target'], output_dict=True)\n","print(report)"],"metadata":{"id":"yerQiLDKPY-t"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Support Vector Machine (SVM)"],"metadata":{"id":"2IztPvxoS8as"}},{"cell_type":"code","source":["from sklearn.svm import SVC\n","from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","clf = make_pipeline(XdawnCovariances(8),\n","                    TangentSpace(metric='logeuclid'),\n","                    SVC(kernel='linear', C=1, decision_function_shape='ovr'))\n","\n","\n","epochs_data = epochs.get_data()\n","labels = epochs.events[:, -1]\n","preds = np.zeros(len(labels))\n","\n","cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n","\n","train_accuracies = []\n","test_accuracies = []\n","train_losses = []\n","test_losses = []\n","preds = np.empty(len(labels))\n","for train_idx, test_idx in cv.split(epochs_data, labels):\n","    clf.fit(epochs_data[train_idx], labels[train_idx])\n","    train_preds = clf.predict(epochs_data[train_idx])\n","    test_preds = clf.predict(epochs_data[test_idx])\n","    train_accuracy = accuracy_score(labels[train_idx], train_preds)\n","    test_accuracy = accuracy_score(labels[test_idx], test_preds)\n","    train_accuracies.append(train_accuracy)\n","    test_accuracies.append(test_accuracy)\n","    train_loss = np.mean(train_preds != labels[train_idx])\n","    test_loss = np.mean(test_preds != labels[test_idx])\n","    train_losses.append(train_loss)\n","    test_losses.append(test_loss)\n","    preds[test_idx] = test_preds\n","report = classification_report(labels, preds, target_names=['non-target', 'target'], output_dict=True)\n","print(report)\n","\n","\n","epochs_range = range(1, len(train_accuracies) + 1)\n","fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n","ax1.plot(epochs_range, train_accuracies, label='Train Accuracy', marker='o')\n","ax1.plot(epochs_range, test_accuracies, label='Test Accuracy', marker='x')\n","ax1.set_title('Train and Test Accuracy')\n","ax1.set_xlabel('Fold')\n","ax1.set_ylabel('Accuracy')\n","ax1.legend()\n","\n","\n","for i in range(len(train_accuracies)):\n","    ax1.annotate(f'{train_accuracies[i]:.2f}', (epochs_range[i], train_accuracies[i]),\n","                 textcoords=\"offset points\", xytext=(0, 10), ha='center', fontsize=8)\n","    ax1.annotate(f'{test_accuracies[i]:.2f}', (epochs_range[i], test_accuracies[i]),\n","                 textcoords=\"offset points\", xytext=(0, 10), ha='center', fontsize=8)\n","\n","ax2.plot(epochs_range, train_losses, label='Train Loss', marker='o')\n","ax2.plot(epochs_range, test_losses, label='Test Loss', marker='x')\n","ax2.set_title('Train and Test Loss')\n","ax2.set_xlabel('Fold')\n","ax2.set_ylabel('Loss')\n","ax2.legend()\n","\n","for i in range(len(train_losses)):\n","    ax2.annotate(f'{train_losses[i]:.2f}', (epochs_range[i], train_losses[i]),\n","                 textcoords=\"offset points\", xytext=(0, 10), ha='center', fontsize=8)\n","    ax2.annotate(f'{test_losses[i]:.2f}', (epochs_range[i], test_losses[i]),\n","                 textcoords=\"offset points\", xytext=(0, 10), ha='center', fontsize=8)\n","\n","plt.tight_layout()\n","plt.show()\n","cm = confusion_matrix(labels, preds)\n","plt.figure(figsize=(6, 5))\n","sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=['non-target', 'target'], yticklabels=['non-target', 'target'])\n","plt.title('Confusion Matrix')\n","plt.xlabel('Predicted')\n","plt.ylabel('True')\n","plt.show()\n"],"metadata":{"id":"LL-Cr7nHR-iD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Random Forest"],"metadata":{"id":"RaJv0OQTTFXX"}},{"cell_type":"code","source":["from sklearn.ensemble import RandomForestClassifier\n","\n","clf = make_pipeline(XdawnCovariances(8),\n","                    TangentSpace(metric='logeuclid'),\n","                    RandomForestClassifier(n_estimators=100, random_state=42))\n","\n","epochs_data = epochs.get_data()\n","labels = epochs.events[:, -1]\n","preds = np.zeros(len(labels))\n","\n","cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n","\n","train_accuracies = []\n","test_accuracies = []\n","train_losses = []\n","test_losses = []\n","\n","preds = np.empty(len(labels))\n","for train_idx, test_idx in cv.split(epochs_data, labels):\n","\n","    clf.fit(epochs_data[train_idx], labels[train_idx])\n","    train_preds = clf.predict(epochs_data[train_idx])\n","    test_preds = clf.predict(epochs_data[test_idx])\n","    train_accuracy = accuracy_score(labels[train_idx], train_preds)\n","    test_accuracy = accuracy_score(labels[test_idx], test_preds)\n","    train_accuracies.append(train_accuracy)\n","    test_accuracies.append(test_accuracy)\n","    train_loss = np.mean(train_preds != labels[train_idx])\n","    test_loss = np.mean(test_preds != labels[test_idx])\n","\n","    train_losses.append(train_loss)\n","    test_losses.append(test_loss)\n","    preds[test_idx] = test_preds\n","\n","report = classification_report(labels, preds, target_names=['non-target', 'target'], output_dict=True)\n","print(report)\n","\n","\n","epochs_range = range(1, len(train_accuracies) + 1)\n","fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n","\n","ax1.plot(epochs_range, train_accuracies, label='Train Accuracy', marker='o')\n","ax1.plot(epochs_range, test_accuracies, label='Test Accuracy', marker='x')\n","ax1.set_title('Train and Test Accuracy')\n","ax1.set_xlabel('Fold')\n","ax1.set_ylabel('Accuracy')\n","ax1.legend()\n","\n","\n","for i in range(len(train_accuracies)):\n","    ax1.annotate(f'{train_accuracies[i]:.2f}', (epochs_range[i], train_accuracies[i]),\n","                 textcoords=\"offset points\", xytext=(0, 10), ha='center', fontsize=8)\n","    ax1.annotate(f'{test_accuracies[i]:.2f}', (epochs_range[i], test_accuracies[i]),\n","                 textcoords=\"offset points\", xytext=(0, 10), ha='center', fontsize=8)\n","\n","ax2.plot(epochs_range, train_losses, label='Train Loss', marker='o')\n","ax2.plot(epochs_range, test_losses, label='Test Loss', marker='x')\n","ax2.set_title('Train and Test Loss')\n","ax2.set_xlabel('Fold')\n","ax2.set_ylabel('Loss')\n","ax2.legend()\n","\n","for i in range(len(train_losses)):\n","    ax2.annotate(f'{train_losses[i]:.2f}', (epochs_range[i], train_losses[i]),\n","                 textcoords=\"offset points\", xytext=(0, 10), ha='center', fontsize=8)\n","    ax2.annotate(f'{test_losses[i]:.2f}', (epochs_range[i], test_losses[i]),\n","                 textcoords=\"offset points\", xytext=(0, 10), ha='center', fontsize=8)\n","\n","plt.tight_layout()\n","plt.show()\n","\n","cm = confusion_matrix(labels, preds)\n","plt.figure(figsize=(6, 5))\n","sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=['non-target', 'target'], yticklabels=['non-target', 'target'])\n","plt.title('Confusion Matrix')\n","plt.xlabel('Predicted')\n","plt.ylabel('True')\n","plt.show()\n"],"metadata":{"id":"-3fTlHmCTEKs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Polynomial Logistic Regression"],"metadata":{"id":"pMEyJg02V2LW"}},{"cell_type":"code","source":["from sklearn.preprocessing import PolynomialFeatures\n","from sklearn.linear_model import LogisticRegression\n","\n","clf = make_pipeline(XdawnCovariances(8),\n","                    TangentSpace(metric='logeuclid'),\n","                    PolynomialFeatures(degree=2, include_bias=False),\n","                    LogisticRegression(penalty='l2', solver='liblinear', multi_class='ovr'))\n","\n","epochs_data = epochs.get_data()\n","labels = epochs.events[:, -1]\n","preds = np.zeros(len(labels))\n","\n","cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n","train_accuracies = []\n","test_accuracies = []\n","train_losses = []\n","test_losses = []\n","\n","preds = np.empty(len(labels))\n","for train_idx, test_idx in cv.split(epochs_data, labels):\n","    clf.fit(epochs_data[train_idx], labels[train_idx])\n","    train_preds = clf.predict(epochs_data[train_idx])\n","    test_preds = clf.predict(epochs_data[test_idx])\n","    train_accuracy = accuracy_score(labels[train_idx], train_preds)\n","    test_accuracy = accuracy_score(labels[test_idx], test_preds)\n","    train_accuracies.append(train_accuracy)\n","    test_accuracies.append(test_accuracy)\n","    train_loss = np.mean(train_preds != labels[train_idx])\n","    test_loss = np.mean(test_preds != labels[test_idx])\n","    train_losses.append(train_loss)\n","    test_losses.append(test_loss)\n","    preds[test_idx] = test_preds\n","\n","\n","report = classification_report(labels, preds, target_names=['non-target', 'target'], output_dict=True)\n","print(report)\n","\n","epochs_range = range(1, len(train_accuracies) + 1)\n","fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n","\n","ax1.plot(epochs_range, train_accuracies, label='Train Accuracy', marker='o')\n","ax1.plot(epochs_range, test_accuracies, label='Test Accuracy', marker='x')\n","ax1.set_title('Train and Test Accuracy')\n","ax1.set_xlabel('Fold')\n","ax1.set_ylabel('Accuracy')\n","ax1.legend()\n","\n","for i in range(len(train_accuracies)):\n","    ax1.annotate(f'{train_accuracies[i]:.2f}', (epochs_range[i], train_accuracies[i]),\n","                 textcoords=\"offset points\", xytext=(0, 10), ha='center', fontsize=8)\n","    ax1.annotate(f'{test_accuracies[i]:.2f}', (epochs_range[i], test_accuracies[i]),\n","                 textcoords=\"offset points\", xytext=(0, 10), ha='center', fontsize=8)\n","ax2.plot(epochs_range, train_losses, label='Train Loss', marker='o')\n","ax2.plot(epochs_range, test_losses, label='Test Loss', marker='x')\n","ax2.set_title('Train and Test Loss')\n","ax2.set_xlabel('Fold')\n","ax2.set_ylabel('Loss')\n","ax2.legend()\n","\n","\n","for i in range(len(train_losses)):\n","    ax2.annotate(f'{train_losses[i]:.2f}', (epochs_range[i], train_losses[i]),\n","                 textcoords=\"offset points\", xytext=(0, 10), ha='center', fontsize=8)\n","    ax2.annotate(f'{test_losses[i]:.2f}', (epochs_range[i], test_losses[i]),\n","                 textcoords=\"offset points\", xytext=(0, 10), ha='center', fontsize=8)\n","plt.tight_layout()\n","plt.show()\n","\n","\n","cm = confusion_matrix(labels, preds)\n","plt.figure(figsize=(6, 5))\n","sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=['non-target', 'target'], yticklabels=['non-target', 'target'])\n","plt.title('Confusion Matrix')\n","plt.xlabel('Predicted')\n","plt.ylabel('True')\n","plt.show()\n"],"metadata":{"id":"-VLso5n3TqLF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"-OGd7NT0XWPk"},"execution_count":null,"outputs":[]}]}
